{
 "cells": [
  {
   "cell_type": "code",
   "id": "0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:08:52.765129Z",
     "start_time": "2025-04-24T10:08:52.630341Z"
    }
   },
   "source": [
    "\n",
    "import shutil\n",
    "shutil.rmtree('__pycache__', ignore_errors=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix\n",
    "import importlib\n",
    "# Our functions\n",
    "from EvalFunctions import AUCEval, MMREval, nDCGEval\n",
    "from RecAlgs import MostPopBaseline, CollaborativeFiltering, Hybrid, News_Recommender_CBF\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "The parameters to select how to tun the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "id": "2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:08:54.179615Z",
     "start_time": "2025-04-24T10:08:54.066517Z"
    }
   },
   "source": [
    "# General\n",
    "TimeCutOffDays = 3              # How old the articles can be that we would consider recommending (older than X days are not considered)\n",
    "\n",
    "# Data selection                \n",
    "TrainingDataStartDate = 0       # From what day we want to collect data\n",
    "TrainingDataWindowSize = 2      # How many days of training data we want, 0 for all\n",
    "TestDataWindowSize = 1          # How many days of test data we want, 0 for all\n",
    "\n",
    "# Algorithm specifics \n",
    "TypeOfRecAlg = 0                # Which RecAlg we want to use 0-Pop, 1-Rand, 2-CBF, 3-CF, 4-Hybrid\n",
    "\n",
    "# Popular Baseline\n",
    "TimePenaltyPerHour = 0.1        # The percentage on penalty per hour the news gets\n",
    "TimePenaltyStart = 24           # After howmany hours in the past the penalty starts\n",
    "\n",
    "# Random Baseline\n",
    "MinScore = 0                    # Minimum score that can be given\n",
    "MaxScore = 1                    # Maximum score that can be given\n",
    "\n",
    "# Content based filtering\n",
    "\n",
    "# Collaborative filtering\n",
    "\n",
    "# Hybrid\n",
    "UsePopBaseline = False          # Whether to use Popularity baseline\n",
    "UseRandBaseLine = False         # Whether to use Random baseline\n",
    "UseCBF = True                   # Whether to use Content based filtering\n",
    "UseCF = True                    # Whether to use Collaborative filtering\n",
    "TakeMax = False                 # Whether to take the max between CBF and CF before applying weights\n",
    "Weights = [0.2, 0.4, 0.4]       # The weights for the different parts (in order of appearance above)\n"
   ],
   "outputs": [],
   "execution_count": 113
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Data selection"
   ]
  },
  {
   "cell_type": "code",
   "id": "90ec24fba2decf9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:08:55.829912Z",
     "start_time": "2025-04-24T10:08:55.717887Z"
    }
   },
   "source": [
    "def getAvailableArticles(GivenTime, AllArticles):\n",
    "    # Ensure the time column is in datetime format\n",
    "    AllArticles['ReleaseDate'] = pd.to_datetime(AllArticles['ReleaseDate'])\n",
    "    GivenTime = pd.to_datetime(GivenTime)\n",
    "\n",
    "    # Filter rows where time is less than or equal to the given time\n",
    "    return AllArticles[AllArticles['ReleaseDate'] >= GivenTime]\n",
    "\n",
    "def getPastBehaviors(GivenTime, AllBehaviors):\n",
    "    # Ensure the time column is in datetime format\n",
    "    AllBehaviors['DateTime'] = pd.to_datetime(AllBehaviors['DateTime'])\n",
    "    GivenTime = pd.to_datetime(GivenTime)\n",
    "\n",
    "    # Filter rows where time is less than or equal to the given time\n",
    "    return AllBehaviors[AllBehaviors['DateTime'] >= GivenTime], AllBehaviors[AllBehaviors['DateTime'] < GivenTime]\n",
    "\n",
    "def getGroundTruth(FutureBehaviors, RequestedUserID):\n",
    "    UserFuture = {row.ClickData for row in FutureBehaviors.itertuples(index=False) if row.UserID == RequestedUserID}\n",
    "    ClickedArticles = []\n",
    "    for ClickData in UserFuture:\n",
    "        for Click in ClickData:\n",
    "            if Click.endswith(\"-1\"):  # Only include clicked articles\n",
    "                ClickedArticles.append(Click.replace(\"-1\", \"\"))\n",
    "    return ClickedArticles\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "id": "4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:09:01.274323Z",
     "start_time": "2025-04-24T10:08:56.457057Z"
    }
   },
   "source": [
    "# Add the first time the article has been seen in the behaviors as the Est_PublishedTime in the articles.\n",
    "AllTrainingData = pd.read_csv(\"../data/MINDsmall_train/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"UserID\", \"DateTime\", \"History\", \"ClickData\"])\n",
    "AllValidationData = pd.read_csv(\"../data/MINDsmall_dev/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"UserID\", \"DateTime\", \"History\", \"ClickData\"])\n",
    "AllData = pd.concat([AllTrainingData, AllValidationData], ignore_index=True)\n",
    "\n",
    "ArticlesTrain = pd.read_csv(\"../data/MINDsmall_train/news.tsv\", sep=\"\\t\", header=None, names=[\"NewsID\", \"Category\", \"SubCategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\"])\n",
    "ArticlesValidation = pd.read_csv(\"../data/MINDsmall_dev/news.tsv\", sep=\"\\t\", header=None, names=[\"NewsID\", \"Category\", \"SubCategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\"])\n",
    "AllArticles = pd.concat([ArticlesTrain, ArticlesValidation], ignore_index=True)\n",
    "\n",
    "ArticlesTrainWithTime = pd.read_csv(\"../data/NewsWithTime/small/TrainNewsWithTime.csv\")\n",
    "ArticlesValidationWithTime = pd.read_csv(\"../data/NewsWithTime/small/DevNewsWithTime.csv\")\n",
    "AllArticlesWithTime = pd.read_csv(\"../data/NewsWithTime/small/AllNewsWithTime.csv\")\n"
   ],
   "outputs": [],
   "execution_count": 115
  },
  {
   "cell_type": "code",
   "id": "771d16d894c0ce7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:09:02.540504Z",
     "start_time": "2025-04-24T10:09:02.426968Z"
    }
   },
   "source": [
    "#Maybe add something to reduce the amount of data??"
   ],
   "outputs": [],
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "id": "9f76e445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:12:19.891021Z",
     "start_time": "2025-04-24T10:10:37.249643Z"
    }
   },
   "source": [
    "colab_filter = CollaborativeFiltering.CollaborativeFiltering(AllTrainingData, epochs=2)\n",
    "\n",
    "colab_filter.initialize()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing collaborative filtering...\n",
      "apply start\n",
      "explode start\n",
      "sparse matrix start\n",
      "Starting ALS using cpu\n",
      "interaction_sparse.shape:  torch.Size([50000, 33196])\n",
      "user_embeddings.shape, item_embeddings.shape:  torch.Size([50000, 3]) torch.Size([33196, 3])\n",
      "training start\n",
      "Epoch 1/2, Loss: 57.45830154418945\n",
      "Epoch 2/2, Loss: 55.3328857421875\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "id": "e21c7bcb7c46e836",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:12:56.936939Z",
     "start_time": "2025-04-24T10:12:46.021359Z"
    }
   },
   "source": [
    "#Main loop\n",
    "TypeOfRecAlg = 2\n",
    "#Assume we use the past behaviors we have to predict the click behavior on the test set (-1's aka clicked articles)\n",
    "#We hope our recommendations include these articles\n",
    "TotalAUCEvalScore = 0\n",
    "TotalMMREEvalScore = 0\n",
    "TotalNDCGEvalScore = 0\n",
    "i=0\n",
    "for _, instance in tqdm(AllValidationData.iterrows(), total=len(AllValidationData), desc=\"Processing Instances\"):\n",
    "    # Get necessary parameters\n",
    "    UserID = instance['UserID']\n",
    "    Time = pd.to_datetime(instance['DateTime'])\n",
    "    AvailableNews = getAvailableArticles(Time, ArticlesValidationWithTime)\n",
    "    PastBehaviors, FutureBehaviors = getPastBehaviors(Time, AllValidationData)\n",
    "    AvailableNews.loc[:, 'ReleaseDate'] = pd.to_datetime(AvailableNews['ReleaseDate'])\n",
    "    # Run the selected RecAlg\n",
    "    if TypeOfRecAlg == 0:\n",
    "        TopTenArticleRecommendations = MostPopBaseline.RecommendMostPopular(AvailableNews, PastBehaviors, \n",
    "                                                                            Time, TimePenaltyPerHour, TimePenaltyStart)\n",
    "    elif TypeOfRecAlg == 1:\n",
    "        path_items = \"../data/MINDsmall_train/news.tsv\"\n",
    "        path_user_behavior = \"../data/MINDsmall_train/behaviors.tsv\"\n",
    "        \n",
    "        recommender = News_Recommender_CBF.NewsRecommenderCBF(path_items, path_user_behavior)\n",
    "        recommender.get_user_frame()\n",
    "        TopTenArticleRecommendations = recommender.recommend(UserID, 10)\n",
    "\n",
    "    elif TypeOfRecAlg == 2:\n",
    "        TopTenArticleRecommendations = colab_filter.getRecommended(UserID, k=10)\n",
    "\n",
    "    elif TypeOfRecAlg == 3:\n",
    "        TopTenArticleRecommendations = Hybrid()\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    TotalAUCEvalScore += AUCEval.AUCEval(TopTenArticleRecommendations, getGroundTruth(FutureBehaviors, UserID))\n",
    "    TotalMMREEvalScore += MMREval.MMREval(TopTenArticleRecommendations, getGroundTruth(FutureBehaviors, UserID))\n",
    "    TotalNDCGEvalScore += nDCGEval.nDCG(TopTenArticleRecommendations, getGroundTruth(FutureBehaviors, UserID))\n",
    "    i+=1\n",
    "\n",
    "\n",
    "AvgAUCScore = TotalAUCEvalScore/i\n",
    "AvgMMREScore = TotalMMREEvalScore/i\n",
    "AvgNDCGScore = TotalNDCGEvalScore/i"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Instances:   0%|          | 23/73152 [00:10<9:13:56,  2.20it/s] \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable numpy.int64 object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[119], line 36\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m---> 36\u001B[0m TotalAUCEvalScore \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mAUCEval\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAUCEval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTopTenArticleRecommendations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgetGroundTruth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mFutureBehaviors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mUserID\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m TotalMMREEvalScore \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m MMREval\u001B[38;5;241m.\u001B[39mMMREval(TopTenArticleRecommendations, getGroundTruth(FutureBehaviors, UserID))\n\u001B[0;32m     38\u001B[0m TotalNDCGEvalScore \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m nDCGEval\u001B[38;5;241m.\u001B[39mnDCG(TopTenArticleRecommendations, getGroundTruth(FutureBehaviors, UserID))\n",
      "File \u001B[1;32m~\\PycharmProjects\\RecommenderSystems\\Program\\EvalFunctions\\AUCEval.py:5\u001B[0m, in \u001B[0;36mAUCEval\u001B[1;34m(Recommendations, GT)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mAUCEval\u001B[39m(Recommendations, GT):\n\u001B[1;32m----> 5\u001B[0m     RelevantGTScores \u001B[38;5;241m=\u001B[39m \u001B[43mGetRelevantGT\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRecommendations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mGT\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     RecommendedScores \u001B[38;5;241m=\u001B[39m [Score \u001B[38;5;28;01mfor\u001B[39;00m _, Score \u001B[38;5;129;01min\u001B[39;00m Recommendations]\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# Check if both 0 and 1 exist in RelevantGTScores\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\RecommenderSystems\\Program\\EvalFunctions\\AUCEval.py:18\u001B[0m, in \u001B[0;36mGetRelevantGT\u001B[1;34m(Recommendations, GT)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mGetRelevantGT\u001B[39m(Recommendations, GT):\n\u001B[0;32m     17\u001B[0m     RelevantGT \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 18\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ID, _ \u001B[38;5;129;01min\u001B[39;00m Recommendations:\n\u001B[0;32m     19\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m ID \u001B[38;5;129;01min\u001B[39;00m GT:\n\u001B[0;32m     20\u001B[0m             RelevantGT\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: cannot unpack non-iterable numpy.int64 object"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "id": "f92b2ae242dfc0d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T09:47:41.744855Z",
     "start_time": "2025-04-24T09:47:41.555812Z"
    }
   },
   "source": [
    "# Average Evaluation\n",
    "\n",
    "# Look at the results, and evaluate them with the different evaluation functions"
   ],
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "57c46e905232c557"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
