{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# Our functions\n",
    "from EvalFunctions import AUCEval, MMREval, nDCGEval\n",
    "import RecAlgs\n",
    "from RecAlgs import CollaborativeFiltering, MostPopBaseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "The parameters to select how to tun the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "TimeCutOffDays = 3              # How old the articles can be that we would consider recommending (older than X days are not considered)\n",
    "\n",
    "# Data selection                \n",
    "TrainingDataStartDate = 0       # From what day we want to collect data\n",
    "TrainingDataWindowSize = 2      # How many days of training data we want, 0 for all\n",
    "TestDataWindowSize = 1          # How many days of test data we want, 0 for all\n",
    "\n",
    "# Algorithm specifics \n",
    "TypeOfRecAlg = 0                # Which RecAlg we want to use 0-Pop, 1-Rand, 2-CBF, 3-CF, 4-Hybrid\n",
    "\n",
    "# Popular Baseline\n",
    "TimePenaltyPerHour = 0.1        # The percentage on penalty per hour the news gets\n",
    "TimePenaltyStart = 24           # After howmany hours in the past the penalty starts\n",
    "\n",
    "# Random Baseline\n",
    "MinScore = 0                    # Minimum score that can be given\n",
    "MaxScore = 1                    # Maximum score that can be given\n",
    "\n",
    "# Content based filtering\n",
    "\n",
    "# Collaborative filtering\n",
    "\n",
    "# Hybrid\n",
    "UsePopBaseline = False          # Whether to use Popularity baseline\n",
    "UseRandBaseLine = False         # Whether to use Random baseline\n",
    "UseCBF = True                   # Whether to use Content based filtering\n",
    "UseCF = True                    # Whether to use Collaborative filtering\n",
    "TakeMax = False                 # Whether to take the max between CBF and CF before applying weights\n",
    "Weights = [0.2, 0.4, 0.4]       # The weights for the different parts (in order of appearance above)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Data selection"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def getAvailableArticles(GivenTime, AllArticles):\n",
    "    # Ensure the time column is in datetime format\n",
    "    AllArticles['ReleaseDate'] = pd.to_datetime(AllArticles['ReleaseDate'])\n",
    "    GivenTime = pd.to_datetime(GivenTime)\n",
    "\n",
    "    # Filter rows where time is less than or equal to the given time\n",
    "    return AllArticles[AllArticles['ReleaseDate'] >= GivenTime]\n",
    "\n",
    "def getPastBehaviors(GivenTime, AllBehaviors):\n",
    "    # Ensure the time column is in datetime format\n",
    "    AllBehaviors['DateTime'] = pd.to_datetime(AllBehaviors['DateTime'])\n",
    "    GivenTime = pd.to_datetime(GivenTime)\n",
    "\n",
    "    # Filter rows where time is less than or equal to the given time\n",
    "    return AllBehaviors[AllBehaviors['DateTime'] >= GivenTime], AllBehaviors[AllBehaviors['DateTime'] < GivenTime]\n",
    "\n",
    "def getGroundTruth(FutureBehaviors, RequestedUserID):\n",
    "    UserFuture = {ClickData for UserID, DateTime, History, ClickData in FutureBehaviors if UserID == RequestedUserID}\n",
    "    ClickedArticles = []\n",
    "    for ClickData in UserFuture:\n",
    "        for Article in ClickData:\n",
    "            if Article.endswith(\"-1\"):\n",
    "                Article = Article.removesuffix(\"-1\")\n",
    "                if Article not in ClickedArticles:\n",
    "                    ClickedArticles.append(Article)\n",
    "                \n",
    "    return ClickedArticles\n",
    "    "
   ],
   "id": "90ec24fba2decf9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the first time the article has been seen in the behaviors as the Est_PublishedTime in the articles.\n",
    "AllTrainingData = pd.read_csv(\"../data/MINDsmall_train/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"UserID\", \"DateTime\", \"History\", \"ClickData\"])\n",
    "AllValidationData = pd.read_csv(\"../data/MINDsmall_dev/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"UserID\", \"DateTime\", \"History\", \"ClickData\"])\n",
    "AllData = pd.concat([AllTrainingData, AllValidationData], ignore_index=True)\n",
    "\n",
    "ArticlesTrain = pd.read_csv(\"../data/MINDsmall_train/news.tsv\", sep=\"\\t\", header=None, names=[\"NewsID\", \"Category\", \"SubCategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\"])\n",
    "ArticlesValidation = pd.read_csv(\"../data/MINDsmall_dev/news.tsv\", sep=\"\\t\", header=None, names=[\"NewsID\", \"Category\", \"SubCategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\"])\n",
    "AllArticles = pd.concat([ArticlesTrain, ArticlesValidation], ignore_index=True)\n",
    "\n",
    "ArticlesTrainWithTime = pd.read_csv(\"../data/NewsWithTime/small/TrainNewsWithTime.csv\")\n",
    "ArticlesValidationWithTime = pd.read_csv(\"../data/NewsWithTime/small/DevNewsWithTime.csv\")\n",
    "AllArticlesWithTime = pd.read_csv(\"../data/NewsWithTime/small/AllNewsWithTime.csv\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#Maybe add something to reduce the amount of data??",
   "id": "771d16d894c0ce7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Main loop\n",
    "#Assume we use the past behaviors we have to predict the click behavior on the test set (-1's aka clicked articles)\n",
    "#We hope our recommendations include these articles\n",
    "TotalAUCEvalScore = 0\n",
    "TotalMMREEvalScore = 0\n",
    "TotalNDCGEvalScore = 0\n",
    "i=0\n",
    "for instance in AllValidationData:\n",
    "    # Get necessary parameters\n",
    "    UserID = instance['UserID']\n",
    "    Time = instance['DateTime']\n",
    "    AvailableNews = getAvailableArticles(Time, ArticlesValidationWithTime)\n",
    "    PastBehaviors, FutureBehaviors = getPastBehaviors(Time, AllValidationData)\n",
    "    \n",
    "    # Run the selected RecAlg\n",
    "    if TypeOfRecAlg == 0:\n",
    "        TopTenArticleRecommendations = MostPopBaseline.ReccomendMostPopular(AvailableNews, PastBehaviors, \n",
    "                                                                            Time, TimePenaltyPerHour, TimePenaltyStart)\n",
    "    elif TypeOfRecAlg == 1:\n",
    "        TopTenArticleRecommendations = RecAlgs.RandomBaseline()\n",
    "    elif TypeOfRecAlg == 2:\n",
    "        TopTenArticleRecommendations = RecAlgs.ContentBasedFiltering()    \n",
    "    elif TypeOfRecAlg == 3:\n",
    "        TopTenArticleRecommendations = RecAlgs.CollaborativeFiltering()\n",
    "    elif TypeOfRecAlg == 4:\n",
    "        TopTenArticleRecommendations = RecAlgs.Hybrid()\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    TotalAUCEvalScore += AUCEval.AUCEval(TopTenArticleRecommendations, getGroundTruth(FutureBehaviors, UserID))\n",
    "    TotalMMREEvalScore += MMREval.MMREval(TopTenArticleRecommendations, getGroundTruth(FutureBehaviors, UserID))\n",
    "    TotalNDCGEvalScore += nDCGEval.nDCG(TopTenArticleRecommendations, getGroundTruth(FutureBehaviors, UserID))\n",
    "    i+=1\n",
    "\n",
    "AvgAUCScore = TotalAUCEvalScore/i\n",
    "AvgMMREScore = TotalMMREEvalScore/i\n",
    "AvgNDCGScore = TotalNDCGEvalScore/i"
   ],
   "id": "e21c7bcb7c46e836"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Average Evaluation\n",
    "\n",
    "# Look at the results, and evaluate them with the different evaluation functions"
   ],
   "id": "f92b2ae242dfc0d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "interaction_matrix = CollaborativeFiltering.initialize(AllTrainingData)\n",
    "\n",
    "\n",
    "# CollaborativeFiltering.getScore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RS)",
   "language": "python",
   "name": "rs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
