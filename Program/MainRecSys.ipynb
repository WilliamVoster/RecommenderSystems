{
 "cells": [
  {
   "cell_type": "code",
   "id": "0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T09:06:27.202674Z",
     "start_time": "2025-04-27T09:06:16.651759Z"
    }
   },
   "source": [
    "\n",
    "import shutil\n",
    "shutil.rmtree('__pycache__', ignore_errors=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix\n",
    "import importlib\n",
    "# Our functions\n",
    "from EvalFunctions import AUCEval, MMREval, nDCGEval\n",
    "from RecAlgs import MostPopBaseline, CollaborativeFiltering, Hybrid, News_Recommender_CBF\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "The parameters to select how to tun the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "id": "2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T09:06:33.779737Z",
     "start_time": "2025-04-27T09:06:32.433038Z"
    }
   },
   "source": [
    "# General\n",
    "TimeCutOffDays = 3              # How old the articles can be that we would consider recommending (older than X days are not considered)\n",
    "AmountToPredict = 10\n",
    "\n",
    "# Data selection                \n",
    "TrainingDataStartDate = 0       # From what day we want to collect data\n",
    "TrainingDataWindowSize = 2      # How many days of training data we want, 0 for all\n",
    "TestDataWindowSize = 1          # How many days of test data we want, 0 for all\n",
    "\n",
    "# Algorithm specifics \n",
    "TypeOfRecAlg = 0                # Which RecAlg we want to use 0-Pop, 1-CBF, 2-CF, 3-Hybrid\n",
    "\n",
    "# Popular Baseline\n",
    "TimePenaltyPerHour = 0.1        # The percentage on penalty per hour the news gets\n",
    "TimePenaltyStart = 24           # After howmany hours in the past the penalty starts\n",
    "\n",
    "# Random Baseline\n",
    "MinScore = 0                    # Minimum score that can be given\n",
    "MaxScore = 1                    # Maximum score that can be given\n",
    "\n",
    "# Content based filtering\n",
    "\n",
    "# Collaborative filtering\n",
    "\n",
    "# Hybrid\n",
    "UsePopBaseline = False          # Whether to use Popularity baseline\n",
    "UseRandBaseLine = False         # Whether to use Random baseline\n",
    "UseCBF = True                   # Whether to use Content based filtering\n",
    "UseCF = True                    # Whether to use Collaborative filtering\n",
    "TakeMax = False                 # Whether to take the max between CBF and CF before applying weights\n",
    "Weights = [0.2, 0.4, 0.4]       # The weights for the different parts (in order of appearance above)\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Data selection"
   ]
  },
  {
   "cell_type": "code",
   "id": "90ec24fba2decf9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T09:06:40.564437Z",
     "start_time": "2025-04-27T09:06:40.454734Z"
    }
   },
   "source": [
    "def getGroundTruth(FutureBehaviors, RequestedUserID, CurrentInstanceClickData):\n",
    "    # Filter only relevant rows\n",
    "    UserData = FutureBehaviors[FutureBehaviors['UserID'] == RequestedUserID]\n",
    "    # Extract clicked articles\n",
    "    ClickedArticles = []\n",
    "    for clicks in UserData['ClickData']:\n",
    "        ClickedArticles.extend(\n",
    "            click.replace(\"-1\", \"\") for click in clicks.split(\" \") if click.endswith(\"-1\")\n",
    "        )\n",
    "    ClickedArticles.extend(\n",
    "        CurrentInstanceClickData.replace(\"-1\", \"\") for aclick in CurrentInstanceClickData.split(\" \") if aclick.endswith(\"-1\")\n",
    "    )\n",
    "    return ClickedArticles\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T09:06:48.067281Z",
     "start_time": "2025-04-27T09:06:43.408953Z"
    }
   },
   "source": [
    "PreparationTime = 0\n",
    "PreparationStartTime = time.time()\n",
    "\n",
    "# Add the first time the article has been seen in the behaviors as the Est_PublishedTime in the articles.\n",
    "AllTrainingData = pd.read_csv(\"../data/MINDsmall_train/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"UserID\", \"DateTime\", \"History\", \"ClickData\"])\n",
    "AllValidationData = pd.read_csv(\"../data/MINDsmall_dev/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"UserID\", \"DateTime\", \"History\", \"ClickData\"])\n",
    "AllData = pd.concat([AllTrainingData, AllValidationData], ignore_index=True)\n",
    "\n",
    "ArticlesTrain = pd.read_csv(\"../data/MINDsmall_train/news.tsv\", sep=\"\\t\", header=None, names=[\"NewsID\", \"Category\", \"SubCategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\"])\n",
    "ArticlesValidation = pd.read_csv(\"../data/MINDsmall_dev/news.tsv\", sep=\"\\t\", header=None, names=[\"NewsID\", \"Category\", \"SubCategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\"])\n",
    "AllArticles = pd.concat([ArticlesTrain, ArticlesValidation], ignore_index=True)\n",
    "\n",
    "ArticlesTrainWithTime = pd.read_csv(\"../data/NewsWithTime/small/TrainNewsWithTime.csv\")\n",
    "ArticlesValidationWithTime = pd.read_csv(\"../data/NewsWithTime/small/DevNewsWithTime.csv\")\n",
    "AllArticlesWithTime = pd.read_csv(\"../data/NewsWithTime/small/AllNewsWithTime.csv\")\n",
    "\n",
    "PreparationEndTime = time.time()\n",
    "PreparationTime += PreparationEndTime - PreparationStartTime"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "771d16d894c0ce7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T09:06:50.826436Z",
     "start_time": "2025-04-27T09:06:50.721288Z"
    }
   },
   "source": [
    "#Maybe add something to reduce the amount of data??"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "9f76e445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T09:06:53.755780Z",
     "start_time": "2025-04-27T09:06:53.653629Z"
    }
   },
   "source": [
    "if TypeOfRecAlg == 2:\n",
    "    PreparationStartTime = time.time()\n",
    "    colab_filter = CollaborativeFiltering.CollaborativeFiltering(AllTrainingData, epochs=10)\n",
    "    \n",
    "    colab_filter.initialize()\n",
    "    PreparationEndTime = time.time()\n",
    "    PreparationTime += PreparationEndTime - PreparationStartTime"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T09:06:55.485951Z",
     "start_time": "2025-04-27T09:06:55.383858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if TypeOfRecAlg == 1:\n",
    "    PreparationStartTime = time.time()\n",
    "    path_items = \"../data/MINDsmall_train/news.tsv\"\n",
    "    path_user_behavior = \"../data/MINDsmall_train/behaviors.tsv\"\n",
    "    \n",
    "    recommender = News_Recommender_CBF.NewsRecommenderCBF(path_items, path_user_behavior)\n",
    "    PreparationEndTime = time.time()\n",
    "    PreparationTime += PreparationEndTime - PreparationStartTime"
   ],
   "id": "90d6b71889329449",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T09:06:59.773453Z",
     "start_time": "2025-04-27T09:06:57.842799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PreparationStartTime = time.time()\n",
    "PopularityDict = {}\n",
    "for row in AllData.itertuples(index=False):\n",
    "    for click in row.ClickData.split(\" \"):  # split string of clicks\n",
    "        if click.endswith(\"-1\"):  # Only clicked articles\n",
    "            ArticleID = click.replace(\"-1\", \"\")\n",
    "            PopularityDict[ArticleID] = PopularityDict.get(ArticleID, 0) + 1\n",
    "PopularityDict = sorted(PopularityDict.items(), key=lambda x: x[1], reverse=True)\n",
    "PreparationEndTime = time.time()\n",
    "PreparationTime += PreparationEndTime - PreparationStartTime"
   ],
   "id": "6896e19c2927f278",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "e21c7bcb7c46e836",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T09:07:09.194618Z",
     "start_time": "2025-04-27T09:07:01.862498Z"
    }
   },
   "source": [
    "PreparationStartTime = time.time()\n",
    "#Main loop\n",
    "#Assume we use the past behaviors we have to predict the click behavior on the test set (-1's aka clicked articles)\n",
    "#We hope our recommendations include these articles\n",
    "TotalAUCEvalScore = 0\n",
    "TotalMMREEvalScore = 0\n",
    "TotalNDCG10EvalScore = 0\n",
    "TotalNDCG5EvalScore = 0\n",
    "i=0\n",
    "amountOfColdStarts = 0\n",
    "TotalInferenceTime = 0\n",
    "\n",
    "# Preprocessing before the loop \n",
    "ArticlesValidationWithTime = ArticlesValidationWithTime.sort_values('ReleaseDate').reset_index(drop=True)\n",
    "ArticlesValidationWithTime['ReleaseDate'] = pd.to_datetime(ArticlesValidationWithTime['ReleaseDate'])\n",
    "ReleaseDates = pd.to_datetime(ArticlesValidationWithTime['ReleaseDate'].values)\n",
    "\n",
    "# Sample set amount of instances in validation\n",
    "AllValidationData = AllValidationData.sample(n=7000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "AllBehaviors = AllValidationData.sort_values('DateTime').reset_index(drop=True)\n",
    "AllBehaviors['DateTime'] = pd.to_datetime(AllBehaviors['DateTime'])\n",
    "AllTimes = pd.to_datetime(AllBehaviors['DateTime'].values)\n",
    "PreparationEndTime = time.time()\n",
    "PreparationTime += PreparationEndTime - PreparationStartTime\n",
    "\n",
    "for _, instance in tqdm(AllValidationData.iterrows(), total=len(AllValidationData), desc=\"Processing Instances\"):\n",
    "    # Start timing inference\n",
    "    InferenceStartTime = time.time()\n",
    "    # Get necessary parameters\n",
    "    UserID = instance['UserID']\n",
    "    Time = pd.to_datetime(instance['DateTime'])\n",
    "    cutoff_index = ReleaseDates.searchsorted(Time, side='right')\n",
    "    AvailableNews = ArticlesValidationWithTime.iloc[:cutoff_index]\n",
    "    \n",
    "    \n",
    "    future_start_idx = AllTimes.searchsorted(Time, side='left')\n",
    "    FutureBehaviors = AllBehaviors.iloc[future_start_idx:]\n",
    "    GT = getGroundTruth(FutureBehaviors, UserID, instance['ClickData'])\n",
    "    \n",
    "    # skip user if there is no future data for this user\n",
    "    if len(GT) == 0:\n",
    "        print(\"skipped user\")\n",
    "        continue\n",
    "        \n",
    "    # Run the selected RecAlg\n",
    "    if TypeOfRecAlg == 0:\n",
    "        # PossibleArticles, CurrentTime, GlobalPopularity, TimePenaltyPerHour, TimePenaltyStart\n",
    "        TopTenArticleRecommendations = MostPopBaseline.RecommendMostPopular(AvailableNews, Time, PopularityDict, TimePenaltyPerHour, TimePenaltyStart, AmountToPredict)\n",
    "    elif TypeOfRecAlg == 1:\n",
    "        TopTenArticleRecommendations = recommender.recommend(UserID, AmountToPredict)\n",
    "\n",
    "    elif TypeOfRecAlg == 2:\n",
    "        TopTenArticleRecommendations = colab_filter.getRecommended(UserID, k=AmountToPredict)\n",
    "\n",
    "    elif TypeOfRecAlg == 3:\n",
    "        PopRec = MostPopBaseline.RecommendMostPopular(AvailableNews, Time, PopularityDict, TimePenaltyPerHour, TimePenaltyStart, -1)\n",
    "        CFRec = colab_filter.getRecommended(UserID, -1)\n",
    "        CBRRec = recommender.recommend(UserID, -1)\n",
    "        TopTenArticleRecommendations = Hybrid.HybridRecommendations(PopRec, CFRec, CBRRec, Weights, AmountToPredict)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    # For cold start\n",
    "    if len(TopTenArticleRecommendations) == 0:\n",
    "        amountOfColdStarts += 1\n",
    "        TopTenArticleRecommendations = MostPopBaseline.RecommendMostPopular(AvailableNews, Time, PopularityDict, TimePenaltyPerHour, TimePenaltyStart, AmountToPredict)\n",
    "        \n",
    "    # End timing inference\n",
    "    InferenceEndTime = time.time()\n",
    "    TotalInferenceTime += (InferenceEndTime - InferenceStartTime)\n",
    "    \n",
    "    # Calculate evaluation scores\n",
    "    AUCScore = AUCEval.AUCEval(TopTenArticleRecommendations, GT)\n",
    "    MMREScore = MMREval.MMREval(TopTenArticleRecommendations, GT)\n",
    "    NDCG10Score = nDCGEval.nDCG(TopTenArticleRecommendations, GT)\n",
    "    NDCG5Score = nDCGEval.nDCG(TopTenArticleRecommendations[:5], GT)\n",
    "    \n",
    "    # Print the scores for the current user and generation\n",
    "    # print(f\"Generation {i}: User {UserID} - AUC: {AUCScore}, MMRE: {MMREScore}, NDCG: {NDCGScore}\")\n",
    "    \n",
    "    # Accumulate the total scores\n",
    "    TotalAUCEvalScore += AUCScore\n",
    "    TotalMMREEvalScore += MMREScore\n",
    "    TotalNDCG10EvalScore += NDCG10Score\n",
    "    TotalNDCG5EvalScore += NDCG5Score\n",
    "    i+=1\n",
    "\n",
    "\n",
    "AvgAUCScore = TotalAUCEvalScore/i\n",
    "AvgMMREScore = TotalMMREEvalScore/i\n",
    "AvgNDCG10Score = TotalNDCG10EvalScore/i\n",
    "AvgNDCG5Score = TotalNDCG5EvalScore/i\n",
    "AvgInferenceTimePerUser = TotalInferenceTime / i\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Instances: 100%|██████████| 100/100 [00:07<00:00, 13.99it/s]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "f92b2ae242dfc0d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T09:07:11.313590Z",
     "start_time": "2025-04-27T09:07:11.208948Z"
    }
   },
   "source": [
    "# Average Evaluation\n",
    "print(f\"Average AUC Score: {AvgAUCScore:.20f}\")\n",
    "print(f\"Average MMRE Score: {AvgMMREScore:.20f}\")\n",
    "print(f\"Average NDCG10 Score: {AvgNDCG10Score:.20f}\")\n",
    "print(f\"Average NDCG5 Score: {AvgNDCG5Score:.20f}\")\n",
    "print(f\"Average Inference Time per User: {AvgInferenceTimePerUser:.6f} seconds\")\n",
    "print(f\"Preparation Time: {PreparationTime:.4f} seconds\")\n",
    "print(f\"Number of Cold Starts: {amountOfColdStarts}\")\n",
    "# Look at the results, and evaluate them with the different evaluation functions"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average AUC Score: 0.19000000000000000222\n",
      "Average MMR Score: 0.04500000000000001221\n",
      "Average NDCG10 Score: 0.23740910153862432308\n",
      "Average NDCG5 Score: 0.20941117485538562892\n",
      "Average Inference Time per User: 0.070379 seconds\n",
      "Preparation Time: 6.4525 seconds\n",
      "Number of Cold Starts: 0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "57c46e905232c557"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
