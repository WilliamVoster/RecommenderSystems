{
 "cells": [
  {
   "cell_type": "code",
   "id": "0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T19:27:43.372826Z",
     "start_time": "2025-04-24T19:27:32.652147Z"
    }
   },
   "source": [
    "\n",
    "import shutil\n",
    "shutil.rmtree('__pycache__', ignore_errors=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix\n",
    "import importlib\n",
    "# Our functions\n",
    "from EvalFunctions import AUCEval, MMREval, nDCGEval\n",
    "from RecAlgs import MostPopBaseline, CollaborativeFiltering, Hybrid, News_Recommender_CBF\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "The parameters to select how to tun the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "id": "2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T19:27:49.277833Z",
     "start_time": "2025-04-24T19:27:47.961030Z"
    }
   },
   "source": [
    "# General\n",
    "TimeCutOffDays = 3              # How old the articles can be that we would consider recommending (older than X days are not considered)\n",
    "\n",
    "# Data selection                \n",
    "TrainingDataStartDate = 0       # From what day we want to collect data\n",
    "TrainingDataWindowSize = 2      # How many days of training data we want, 0 for all\n",
    "TestDataWindowSize = 1          # How many days of test data we want, 0 for all\n",
    "\n",
    "# Algorithm specifics \n",
    "TypeOfRecAlg = 0                # Which RecAlg we want to use 0-Pop, 1-Rand, 2-CBF, 3-CF, 4-Hybrid\n",
    "\n",
    "# Popular Baseline\n",
    "TimePenaltyPerHour = 0.1        # The percentage on penalty per hour the news gets\n",
    "TimePenaltyStart = 24           # After howmany hours in the past the penalty starts\n",
    "\n",
    "# Random Baseline\n",
    "MinScore = 0                    # Minimum score that can be given\n",
    "MaxScore = 1                    # Maximum score that can be given\n",
    "\n",
    "# Content based filtering\n",
    "\n",
    "# Collaborative filtering\n",
    "\n",
    "# Hybrid\n",
    "UsePopBaseline = False          # Whether to use Popularity baseline\n",
    "UseRandBaseLine = False         # Whether to use Random baseline\n",
    "UseCBF = True                   # Whether to use Content based filtering\n",
    "UseCF = True                    # Whether to use Collaborative filtering\n",
    "TakeMax = False                 # Whether to take the max between CBF and CF before applying weights\n",
    "Weights = [0.2, 0.4, 0.4]       # The weights for the different parts (in order of appearance above)\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Data selection"
   ]
  },
  {
   "cell_type": "code",
   "id": "90ec24fba2decf9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:25:19.909847Z",
     "start_time": "2025-04-24T20:25:19.804866Z"
    }
   },
   "source": [
    "def getGroundTruth(FutureBehaviors, RequestedUserID, CurrentInstanceClickData):\n",
    "    # Filter only relevant rows\n",
    "    UserData = FutureBehaviors[FutureBehaviors['UserID'] == RequestedUserID]\n",
    "    # Extract clicked articles\n",
    "    ClickedArticles = []\n",
    "    for clicks in UserData['ClickData']:\n",
    "        ClickedArticles.extend(\n",
    "            click.replace(\"-1\", \"\") for click in clicks.split(\" \") if click.endswith(\"-1\")\n",
    "        )\n",
    "    ClickedArticles.extend(\n",
    "        CurrentInstanceClickData.replace(\"-1\", \"\") for aclick in CurrentInstanceClickData.split(\" \") if aclick.endswith(\"-1\")\n",
    "    )\n",
    "    return ClickedArticles\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "id": "4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T19:28:01.901192Z",
     "start_time": "2025-04-24T19:27:57.118053Z"
    }
   },
   "source": [
    "# Add the first time the article has been seen in the behaviors as the Est_PublishedTime in the articles.\n",
    "AllTrainingData = pd.read_csv(\"../data/MINDsmall_train/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"UserID\", \"DateTime\", \"History\", \"ClickData\"])\n",
    "AllValidationData = pd.read_csv(\"../data/MINDsmall_dev/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"UserID\", \"DateTime\", \"History\", \"ClickData\"])\n",
    "AllData = pd.concat([AllTrainingData, AllValidationData], ignore_index=True)\n",
    "\n",
    "ArticlesTrain = pd.read_csv(\"../data/MINDsmall_train/news.tsv\", sep=\"\\t\", header=None, names=[\"NewsID\", \"Category\", \"SubCategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\"])\n",
    "ArticlesValidation = pd.read_csv(\"../data/MINDsmall_dev/news.tsv\", sep=\"\\t\", header=None, names=[\"NewsID\", \"Category\", \"SubCategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\"])\n",
    "AllArticles = pd.concat([ArticlesTrain, ArticlesValidation], ignore_index=True)\n",
    "\n",
    "ArticlesTrainWithTime = pd.read_csv(\"../data/NewsWithTime/small/TrainNewsWithTime.csv\")\n",
    "ArticlesValidationWithTime = pd.read_csv(\"../data/NewsWithTime/small/DevNewsWithTime.csv\")\n",
    "AllArticlesWithTime = pd.read_csv(\"../data/NewsWithTime/small/AllNewsWithTime.csv\")\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "771d16d894c0ce7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T19:28:07.418819Z",
     "start_time": "2025-04-24T19:28:07.315882Z"
    }
   },
   "source": [
    "#Maybe add something to reduce the amount of data??"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "9f76e445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:29:19.900804Z",
     "start_time": "2025-04-24T20:27:13.160709Z"
    }
   },
   "source": [
    "colab_filter = CollaborativeFiltering.CollaborativeFiltering(AllTrainingData, epochs=2)\n",
    "\n",
    "colab_filter.initialize()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing collaborative filtering...\n",
      "apply start\n",
      "explode start\n",
      "sparse matrix start\n",
      "Starting ALS using cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Storm\\PycharmProjects\\RecommenderSystems\\Program\\RecAlgs\\CollaborativeFiltering.py:78: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  indices=torch.tensor([rows, cols], device=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction_sparse.shape:  torch.Size([50000, 33196])\n",
      "user_embeddings.shape, item_embeddings.shape:  torch.Size([50000, 3]) torch.Size([33196, 3])\n",
      "training start\n",
      "Epoch 1/2, Loss: 57.4981689453125\n",
      "Epoch 2/2, Loss: 55.384742736816406\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T19:28:17.188642Z",
     "start_time": "2025-04-24T19:28:09.339467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path_items = \"../data/MINDsmall_train/news.tsv\"\n",
    "path_user_behavior = \"../data/MINDsmall_train/behaviors.tsv\"\n",
    "\n",
    "recommender = News_Recommender_CBF.NewsRecommenderCBF(path_items, path_user_behavior)\n",
    "recommender.get_user_frame()"
   ],
   "id": "90d6b71889329449",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 ----------> ITEM data loaded successfully: 51282 records!\n",
      "01 ----------> USER data loaded successfully: 156965 records!\n",
      "02 ----------> Corpus created: 51282 documents!\n",
      "03 ----------> TF-IDF matrix created: 51282 documents, 167113 terms!\n",
      "04 ----------> Category matrix created: 51282 documents, 281 categories!\n",
      "05 ----------> Combined matrix created, shape: (51282, 167394)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "           uID                   t  \\\n",
       "1       U13740 2019-11-11 09:05:58   \n",
       "2       U91836 2019-11-12 18:11:30   \n",
       "3       U73700 2019-11-14 07:01:48   \n",
       "4       U34670 2019-11-11 05:28:05   \n",
       "5        U8125 2019-11-12 16:11:21   \n",
       "...        ...                 ...   \n",
       "156961  U21593 2019-11-14 22:24:05   \n",
       "156962  U10123 2019-11-13 06:57:04   \n",
       "156963  U75630 2019-11-14 10:58:13   \n",
       "156964  U44625 2019-11-13 14:57:02   \n",
       "156965  U64800 2019-11-14 15:25:49   \n",
       "\n",
       "                                                ClickHist  \\\n",
       "1       N55189 N42782 N34694 N45794 N18445 N63302 N104...   \n",
       "2       N31739 N6072 N63045 N23979 N35656 N43353 N8129...   \n",
       "3       N10732 N25792 N7563 N21087 N41087 N5445 N60384...   \n",
       "4       N45729 N2203 N871 N53880 N41375 N43142 N33013 ...   \n",
       "5                             N10078 N56514 N14904 N33740   \n",
       "...                                                   ...   \n",
       "156961  N7432 N58559 N1954 N43353 N14343 N13008 N28833...   \n",
       "156962  N9803 N104 N24462 N57318 N55743 N40526 N31726 ...   \n",
       "156963  N29898 N59704 N4408 N9803 N53644 N26103 N812 N...   \n",
       "156964  N4118 N47297 N3164 N43295 N6056 N38747 N42973 ...   \n",
       "156965                                      N22997 N48742   \n",
       "\n",
       "                                                   ImpLog  \n",
       "1                                       N55689-1 N35729-0  \n",
       "2       N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...  \n",
       "3       N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...  \n",
       "4                     N35729-0 N33632-0 N49685-1 N27581-0  \n",
       "5       N39985-0 N36050-0 N16096-0 N8400-1 N22407-0 N6...  \n",
       "...                                                   ...  \n",
       "156961  N2235-0 N22975-0 N64037-0 N47652-0 N11378-0 N4...  \n",
       "156962  N3841-0 N61571-0 N58813-0 N28213-0 N4428-0 N25...  \n",
       "156963  N55913-0 N62318-0 N53515-0 N10960-0 N9135-0 N5...  \n",
       "156964  N6219-0 N3663-0 N31147-0 N58363-0 N4107-0 N457...  \n",
       "156965                N61233-0 N33828-1 N19661-0 N41934-0  \n",
       "\n",
       "[156965 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uID</th>\n",
       "      <th>t</th>\n",
       "      <th>ClickHist</th>\n",
       "      <th>ImpLog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U13740</td>\n",
       "      <td>2019-11-11 09:05:58</td>\n",
       "      <td>N55189 N42782 N34694 N45794 N18445 N63302 N104...</td>\n",
       "      <td>N55689-1 N35729-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U91836</td>\n",
       "      <td>2019-11-12 18:11:30</td>\n",
       "      <td>N31739 N6072 N63045 N23979 N35656 N43353 N8129...</td>\n",
       "      <td>N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U73700</td>\n",
       "      <td>2019-11-14 07:01:48</td>\n",
       "      <td>N10732 N25792 N7563 N21087 N41087 N5445 N60384...</td>\n",
       "      <td>N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U34670</td>\n",
       "      <td>2019-11-11 05:28:05</td>\n",
       "      <td>N45729 N2203 N871 N53880 N41375 N43142 N33013 ...</td>\n",
       "      <td>N35729-0 N33632-0 N49685-1 N27581-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>U8125</td>\n",
       "      <td>2019-11-12 16:11:21</td>\n",
       "      <td>N10078 N56514 N14904 N33740</td>\n",
       "      <td>N39985-0 N36050-0 N16096-0 N8400-1 N22407-0 N6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156961</th>\n",
       "      <td>U21593</td>\n",
       "      <td>2019-11-14 22:24:05</td>\n",
       "      <td>N7432 N58559 N1954 N43353 N14343 N13008 N28833...</td>\n",
       "      <td>N2235-0 N22975-0 N64037-0 N47652-0 N11378-0 N4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156962</th>\n",
       "      <td>U10123</td>\n",
       "      <td>2019-11-13 06:57:04</td>\n",
       "      <td>N9803 N104 N24462 N57318 N55743 N40526 N31726 ...</td>\n",
       "      <td>N3841-0 N61571-0 N58813-0 N28213-0 N4428-0 N25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156963</th>\n",
       "      <td>U75630</td>\n",
       "      <td>2019-11-14 10:58:13</td>\n",
       "      <td>N29898 N59704 N4408 N9803 N53644 N26103 N812 N...</td>\n",
       "      <td>N55913-0 N62318-0 N53515-0 N10960-0 N9135-0 N5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156964</th>\n",
       "      <td>U44625</td>\n",
       "      <td>2019-11-13 14:57:02</td>\n",
       "      <td>N4118 N47297 N3164 N43295 N6056 N38747 N42973 ...</td>\n",
       "      <td>N6219-0 N3663-0 N31147-0 N58363-0 N4107-0 N457...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156965</th>\n",
       "      <td>U64800</td>\n",
       "      <td>2019-11-14 15:25:49</td>\n",
       "      <td>N22997 N48742</td>\n",
       "      <td>N61233-0 N33828-1 N19661-0 N41934-0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156965 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T19:28:24.944554Z",
     "start_time": "2025-04-24T19:28:23.126766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PopularityDict = {}\n",
    "for row in AllData.itertuples(index=False):\n",
    "    for click in row.ClickData.split(\" \"):  # split string of clicks\n",
    "        if click.endswith(\"-1\"):  # Only clicked articles\n",
    "            ArticleID = click.replace(\"-1\", \"\")\n",
    "            PopularityDict[ArticleID] = PopularityDict.get(ArticleID, 0) + 1\n",
    "PopularityDict = sorted(PopularityDict.items(), key=lambda x: x[1], reverse=True)\n"
   ],
   "id": "6896e19c2927f278",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T19:46:53.114991Z",
     "start_time": "2025-04-24T19:46:52.949747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_clickdata(df):\n",
    "    # Check if 'ClickData' exists\n",
    "    if 'ClickData' not in df.columns:\n",
    "        print(\"ClickData column is missing\")\n",
    "        return False\n",
    "\n",
    "    # Check for missing or NaN values\n",
    "    missing_data = df['ClickData'].isna().sum()\n",
    "    if missing_data > 0:\n",
    "        print(f\"There are {missing_data} rows with missing 'ClickData'\")\n",
    "        return False\n",
    "\n",
    "    # Check for empty strings in 'ClickData'\n",
    "    empty_data = (df['ClickData'].apply(lambda x: isinstance(x, str) and x.strip() == '')).sum()\n",
    "    if empty_data > 0:\n",
    "        print(f\"There are {empty_data} rows with empty 'ClickData'\")\n",
    "        return False\n",
    "\n",
    "    print(\"All rows have valid 'ClickData'\")\n",
    "    return True"
   ],
   "id": "d14acd28de9b76dd",
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "e21c7bcb7c46e836",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:30:32.343471Z",
     "start_time": "2025-04-24T20:29:53.406836Z"
    }
   },
   "source": [
    "#Main loop\n",
    "TypeOfRecAlg = 2\n",
    "#Assume we use the past behaviors we have to predict the click behavior on the test set (-1's aka clicked articles)\n",
    "#We hope our recommendations include these articles\n",
    "TotalAUCEvalScore = 0\n",
    "TotalMMREEvalScore = 0\n",
    "TotalNDCGEvalScore = 0\n",
    "i=0\n",
    "amountOfColdStarts = 0\n",
    "\n",
    "# Preprocessing before the loop \n",
    "ArticlesValidationWithTime = ArticlesValidationWithTime.sort_values('ReleaseDate').reset_index(drop=True)\n",
    "ArticlesValidationWithTime['ReleaseDate'] = pd.to_datetime(ArticlesValidationWithTime['ReleaseDate'])\n",
    "ReleaseDates = pd.to_datetime(ArticlesValidationWithTime['ReleaseDate'].values)\n",
    "\n",
    "AllBehaviors = AllValidationData.sort_values('DateTime').reset_index(drop=True)\n",
    "AllBehaviors['DateTime'] = pd.to_datetime(AllBehaviors['DateTime'])\n",
    "AllTimes = pd.to_datetime(AllBehaviors['DateTime'].values)\n",
    "\n",
    "for _, instance in tqdm(AllValidationData.iterrows(), total=len(AllValidationData), desc=\"Processing Instances\"):\n",
    "    # Get necessary parameters\n",
    "    UserID = instance['UserID']\n",
    "    Time = pd.to_datetime(instance['DateTime'])\n",
    "    cutoff_index = ReleaseDates.searchsorted(Time, side='right')\n",
    "    AvailableNews = ArticlesValidationWithTime.iloc[:cutoff_index]\n",
    "    \n",
    "    \n",
    "    future_start_idx = AllTimes.searchsorted(Time, side='left')\n",
    "    FutureBehaviors = AllBehaviors.iloc[future_start_idx:]\n",
    "    GT = getGroundTruth(FutureBehaviors, UserID, instance['ClickData'])\n",
    "    \n",
    "    # skip user if there is no future data for this user\n",
    "    if len(GT) == 0:\n",
    "        print(\"skipped user\")\n",
    "        continue\n",
    "        \n",
    "    # Run the selected RecAlg\n",
    "    if TypeOfRecAlg == 0:\n",
    "        # PossibleArticles, CurrentTime, GlobalPopularity, TimePenaltyPerHour, TimePenaltyStart\n",
    "        TopTenArticleRecommendations = MostPopBaseline.RecommendMostPopular(AvailableNews, Time, PopularityDict, TimePenaltyPerHour, TimePenaltyStart)\n",
    "    elif TypeOfRecAlg == 1:\n",
    "        TopTenArticleRecommendations = recommender.recommend(UserID, 10)\n",
    "\n",
    "    elif TypeOfRecAlg == 2:\n",
    "        TopTenArticleRecommendations = colab_filter.getRecommended(UserID, k=10)\n",
    "\n",
    "    # elif TypeOfRecAlg == 3:\n",
    "    #     TopTenArticleRecommendations = Hybrid()\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    # For cold start\n",
    "    if len(TopTenArticleRecommendations) == 0:\n",
    "        amountOfColdStarts += 1\n",
    "        TopTenArticleRecommendations = MostPopBaseline.RecommendMostPopular(AvailableNews, Time, PopularityDict, TimePenaltyPerHour, TimePenaltyStart)\n",
    "        \n",
    "    # Calculate evaluation scores\n",
    "    AUCScore = AUCEval.AUCEval(TopTenArticleRecommendations, GT)\n",
    "    MMREScore = MMREval.MMREval(TopTenArticleRecommendations, GT)\n",
    "    NDCGScore = nDCGEval.nDCG(TopTenArticleRecommendations, GT)\n",
    "    \n",
    "    # Print the scores for the current user and generation\n",
    "    # print(f\"Generation {i}: User {UserID} - AUC: {AUCScore}, MMRE: {MMREScore}, NDCG: {NDCGScore}\")\n",
    "    \n",
    "    # Accumulate the total scores\n",
    "    TotalAUCEvalScore += AUCScore\n",
    "    TotalMMREEvalScore += MMREScore\n",
    "    TotalNDCGEvalScore += NDCGScore\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "\n",
    "AvgAUCScore = TotalAUCEvalScore/i\n",
    "AvgMMREScore = TotalMMREEvalScore/i\n",
    "AvgNDCGScore = TotalNDCGEvalScore/i\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Instances:   1%|          | 451/73152 [00:36<1:38:52, 12.26it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[74], line 55\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(TopTenArticleRecommendations) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     54\u001B[0m     amountOfColdStarts \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m---> 55\u001B[0m     TopTenArticleRecommendations \u001B[38;5;241m=\u001B[39m \u001B[43mMostPopBaseline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mRecommendMostPopular\u001B[49m\u001B[43m(\u001B[49m\u001B[43mAvailableNews\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTime\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mPopularityDict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTimePenaltyPerHour\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTimePenaltyStart\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# Calculate evaluation scores\u001B[39;00m\n\u001B[0;32m     58\u001B[0m AUCScore \u001B[38;5;241m=\u001B[39m AUCEval\u001B[38;5;241m.\u001B[39mAUCEval(TopTenArticleRecommendations, GT)\n",
      "File \u001B[1;32m~\\PycharmProjects\\RecommenderSystems\\Program\\RecAlgs\\MostPopBaseline.py:3\u001B[0m, in \u001B[0;36mRecommendMostPopular\u001B[1;34m(PossibleArticles, CurrentTime, GlobalPopularitySorted, TimePenaltyPerHour, TimePenaltyStart)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mRecommendMostPopular\u001B[39m(PossibleArticles, CurrentTime, GlobalPopularitySorted, TimePenaltyPerHour, TimePenaltyStart):\n\u001B[0;32m      2\u001B[0m     AvailableIDs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(PossibleArticles[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNewsID\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m----> 3\u001B[0m     ReleaseDates \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(PossibleArticles[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNewsID\u001B[39m\u001B[38;5;124m\"\u001B[39m], PossibleArticles[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReleaseDate\u001B[39m\u001B[38;5;124m\"\u001B[39m]))\n\u001B[0;32m      5\u001B[0m     Recommendations \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      6\u001B[0m     tried \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:666\u001B[0m, in \u001B[0;36mDatetimeArray.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    664\u001B[0m start_i \u001B[38;5;241m=\u001B[39m i \u001B[38;5;241m*\u001B[39m chunksize\n\u001B[0;32m    665\u001B[0m end_i \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m((i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m chunksize, length)\n\u001B[1;32m--> 666\u001B[0m converted \u001B[38;5;241m=\u001B[39m \u001B[43mints_to_pydatetime\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    667\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mstart_i\u001B[49m\u001B[43m:\u001B[49m\u001B[43mend_i\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    668\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtz\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtz\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbox\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtimestamp\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    670\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreso\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_creso\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    671\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    672\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m converted\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "id": "f92b2ae242dfc0d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T09:47:41.744855Z",
     "start_time": "2025-04-24T09:47:41.555812Z"
    }
   },
   "source": [
    "# Average Evaluation\n",
    "\n",
    "# Look at the results, and evaluate them with the different evaluation functions"
   ],
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "57c46e905232c557"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
