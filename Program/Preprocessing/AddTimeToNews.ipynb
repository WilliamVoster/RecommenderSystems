{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Add the first time the article has been seen in the behaviors as the Est_PublishedTime in the articles.\n",
    "AllTrainingData = pd.read_csv(\"../data/MINDsmall_train/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"UserID\", \"DateTime\", \"History\", \"ClickData\"])\n",
    "AllValidationData = pd.read_csv(\"../data/MINDsmall_dev/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"UserID\", \"DateTime\", \"History\", \"ClickData\"])\n",
    "AllData = pd.concat([AllTrainingData, AllValidationData], ignore_index=True)\n",
    "\n",
    "ArticlesTrain = pd.read_csv(\"../data/MINDsmall_train/news.tsv\", sep=\"\\t\", header=None, names=[\"NewsID\", \"Category\", \"SubCategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\"])\n",
    "ArticlesValidation = pd.read_csv(\"../data/MINDsmall_dev/news.tsv\", sep=\"\\t\", header=None, names=[\"NewsID\", \"Category\", \"SubCategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\"])\n",
    "AllArticles = pd.concat([ArticlesTrain, ArticlesValidation], ignore_index=True)"
   ],
   "id": "ae8818f08acb5c51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_sparse_matrix(behaviors):\n",
    "\n",
    "  behaviors['History'] = behaviors['History'].str.split()\n",
    "\n",
    "  # behaviors.reset_index(inplace=True)\n",
    "\n",
    "  # Create new column with list of user ID and time stamp\n",
    "  behaviors['UserIDDateTime'] = behaviors['UserID'].astype(str) + ', ' + behaviors['DateTime'].astype(str)\n",
    "\n",
    "  # Generate a list of unique articles from the 'articles_read' column\n",
    "  articles = sorted(set(article for history in behaviors['History'] if isinstance(history, list) for article in history))\n",
    "\n",
    "  # Create a dictionary to map article IDs to column indices\n",
    "  article_to_index = {article: index for index, article in enumerate(articles)}\n",
    "\n",
    "  # Create a dictionary to map user ID-timestamp combinations to indices\n",
    "  user_id_timestamps = sorted(behaviors['UserIDDateTime'].unique())\n",
    "  user_to_index = {user_id_timestamps: index for index, user_id_timestamps in enumerate(user_id_timestamps)}\n",
    "\n",
    "  # Create an empty sparse matrix with boolean values\n",
    "  num_articles = len(articles)\n",
    "  num_users = len(user_id_timestamps)\n",
    "  sparse_matrix = lil_matrix((num_users, num_articles), dtype=bool)\n",
    "\n",
    "  # Fill the sparse matrix with user interactions\n",
    "  for _, row in behaviors.iterrows():\n",
    "      user_id = row['UserIDDateTime']\n",
    "      user_index = user_to_index[user_id]  # Assuming user IDs start with 'user' followed by a number\n",
    "      history = row['History']\n",
    "      if isinstance(history, list):\n",
    "          for article in history:\n",
    "              if article in article_to_index:\n",
    "                  article_index = article_to_index[article]\n",
    "                  sparse_matrix[user_index, article_index] = True\n",
    "      else:\n",
    "          if history in article_to_index:\n",
    "              article_index = article_to_index[history]\n",
    "              sparse_matrix[user_index, article_index] = True\n",
    "\n",
    "  # Convert the sparse matrix to a DataFrame\n",
    "  data_sparse = pd.DataFrame.sparse.from_spmatrix(sparse_matrix, columns=articles, index=user_id_timestamps)\n",
    "\n",
    "  data_sparse.reset_index(inplace=True)\n",
    "\n",
    "  # Extracting user id and timestamp from combined index column\n",
    "  data_sparse['UserID'] = data_sparse['index'].str.split(',').str[0]\n",
    "  data_sparse['DateTime'] = data_sparse['index'].str.split(',').str[1]\n",
    "\n",
    "  # Dropping combined user_id & timestamp index column\n",
    "  data_sparse.drop(columns='index',inplace=True)\n",
    "\n",
    "  # Moving the last two columns to the front\n",
    "  # Get the column names of the last two columns\n",
    "  last_two_columns = data_sparse.columns[-2:]\n",
    "  # Recreating dataframe\n",
    "  data_sparse = data_sparse[last_two_columns.tolist() + data_sparse.columns[:-2].tolist()]\n",
    "\n",
    "  return data_sparse"
   ],
   "id": "e74ed9cd9afcd204"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_release_date(data_sparse, news):\n",
    "  # Sort the data_sparse DataFrame by the timestamp column\n",
    "  sorted_sparse_df = data_sparse.sort_values(by='DateTime')\n",
    "\n",
    "  # Find the release date of each news article\n",
    "  release_dates = {}\n",
    "\n",
    "  # Iterate over the columns of data_sparse starting from the third column\n",
    "  for col in sorted_sparse_df.columns[2:]:\n",
    "      # Find the first index where the column has a value of 1\n",
    "      first_nonzero_index = sorted_sparse_df[col].idxmax()\n",
    "\n",
    "      # Get the timestamp from the sorted_sparse_df using the first_nonzero_index\n",
    "      timestamp = sorted_sparse_df.loc[first_nonzero_index, 'DateTime']\n",
    "\n",
    "      # Store the release date in the release_dates dictionary with the article ID as the key\n",
    "      release_dates[col] = timestamp\n",
    "\n",
    "  # Convert the release_dates dictionary into a DataFrame\n",
    "  release_dates_df = pd.DataFrame(release_dates.items(), columns=['NewsID', 'ReleaseDate'])\n",
    "\n",
    "  # Join the two dataframes based on the 'news_id' column using left join\n",
    "  news_dates = news.merge(release_dates_df, on='NewsID', how='left')\n",
    "\n",
    "  news_dates['ReleaseDate'] = pd.to_datetime(news_dates['ReleaseDate'], errors='coerce')\n",
    "\n",
    "  # Fill the nulls with the first date\n",
    "  news_dates['ReleaseDate'].fillna(news_dates['ReleaseDate'].min(), inplace=True)\n",
    "\n",
    "  return news_dates"
   ],
   "id": "fb9d5658105069b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "SparseData = get_sparse_matrix(AllData)",
   "id": "9d7231ef7dc0129a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "AllArticlesWithTime = get_release_date(SparseData, AllArticles)\n",
    "AllArticlesWithTime.to_csv(\"../data/NewsWithTime/small/AllNewsWithTime.csv\", index=False, sep=\",\")\n",
    "AllArticlesWithTime.head(3)"
   ],
   "id": "1fd207cc0c7eaf05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "TrainArticlesWithTime = get_release_date(SparseData, ArticlesTrain)\n",
    "TrainArticlesWithTime.to_csv(\"../data/NewsWithTime/small/TrainNewsWithTime.csv\", index=False, sep=\",\")\n",
    "TrainArticlesWithTime.head(3)"
   ],
   "id": "f72902c6d280113e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ValArticlesWithTime = get_release_date(SparseData, ArticlesValidation)\n",
    "ValArticlesWithTime.to_csv(\"../data/NewsWithTime/small/DevNewsWithTime.csv\", index=False, sep=\",\")\n",
    "ValArticlesWithTime.head(3)"
   ],
   "id": "34ad2ca6321e4d61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
